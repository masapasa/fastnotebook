{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UCyCMPcvLGw"
      },
      "source": [
        "# Text-to-Image Search via CLIP\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1yKnmy2Qotrh3OhgwWRsMWPFwOSAecBxg?usp=sharing\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n",
        "\n",
        "Traditionally, searching images from text (text-image-retrieval) relies heavily on human annotations, this is commonly referred to as *Text/Tag based Image Retrieval (TBIR)*.\n",
        "\n",
        "The [OpenAI CLIP](https://github.com/openai/CLIP) model maps the dense vectors extracted from text and image into the same semantic space and produces a strong zero-shot model to measure the similarity between text and images.\n",
        "\n",
        "This guide will showcase fine-tuning a `CLIP` model for text to image retrieval.\n",
        "\n",
        "*Note, please consider switching to GPU/TPU Runtime for faster inference.*\n",
        "\n",
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vglobi-vvqCd"
      },
      "outputs": [],
      "source": [
        "!pip install 'finetuner[full]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXddluSIwCGW"
      },
      "source": [
        "## Task\n",
        "We'll be fine-tuning CLIP on the [fashion captioning dataset](https://github.com/xuewyang/Fashion_Captioning) which contains information about fashion products.\n",
        "\n",
        "For each product the dataset contains a title and images of multiple variants of the product. We constructed a parent [`Document`](https://docarray.jina.ai/fundamentals/document/#document) for each picture, which contains two [chunks](https://docarray.jina.ai/fundamentals/document/nested/#nested-structure): an image document and a text document holding the description of the product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVBez7dHwIye"
      },
      "source": [
        "## Data\n",
        "Our journey starts locally. We have to [prepare the data and push it to the Jina AI Cloud](https://finetuner.jina.ai/walkthrough/create-training-data/) and Finetuner will be able to get the dataset by its name. For this example,\n",
        "we already prepared the data, and we'll provide the names of training and evaluation data (`fashion-train-data-clip` and `fashion-eval-data-clip`) directly to Finetuner.\n",
        "\n",
        "```{admonition} \n",
        "We don't require you to push data to the Jina AI Cloud by yourself. Instead of a name, you can provide a `DocumentArray` and Finetuner will do the job for you.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vfPZBQVxxEHm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/aswin/anaconda3/envs/fast/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c985003617d840faa96aefadf4e127a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(VBox(children=(HTML(value=\"\\n<div class='custom-container'>\\n    <style>\\n        .button1 {\\n â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import finetuner\n",
        "from docarray import DocumentArray, Document\n",
        "\n",
        "finetuner.login(force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cpIj7viExFti"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸ” <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">You are logged in to Jina AI</span> as <span style=\"font-weight: bold\">masapasa</span>. To log out, use <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">jina auth logout</span>.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "ğŸ” \u001b[1;32mYou are logged in to Jina AI\u001b[0m as \u001b[1mmasapasa\u001b[0m. To log out, use \u001b[2mjina auth logout\u001b[0m.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f08a2422fe304e879466d573af2ec933",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "904fcd5bfcaa4859bc48cab5f173a0e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Documents Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚                                                     â”‚\n",
              "â”‚   Type                      DocumentArrayInMemory   â”‚\n",
              "â”‚   Length                    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12514</span>                   â”‚\n",
              "â”‚   Homogenous Documents      <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>                    â”‚\n",
              "â”‚   Has nested Documents in   <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'chunks'</span>,<span style=\"font-weight: bold\">)</span>             â”‚\n",
              "â”‚   Common Attributes         <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chunks'</span><span style=\"font-weight: bold\">)</span>        â”‚\n",
              "â”‚   Multimodal dataclass      <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>                   â”‚\n",
              "â”‚                                                     â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Attributes Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚                                                                    â”‚\n",
              "â”‚  <span style=\"font-weight: bold\"> Attribute </span> <span style=\"font-weight: bold\"> Data type       </span> <span style=\"font-weight: bold\"> #Unique values </span> <span style=\"font-weight: bold\"> Has empty value </span>  â”‚\n",
              "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\n",
              "â”‚   chunks      <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'ChunkArray'</span>,<span style=\"font-weight: bold\">)</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12514</span>            <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>             â”‚\n",
              "â”‚   id          <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span>,<span style=\"font-weight: bold\">)</span>          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12514</span>            <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>             â”‚\n",
              "â”‚                                                                    â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Documents Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚                                                     â”‚\n",
              "â”‚   Type                      DocumentArrayInMemory   â”‚\n",
              "â”‚   Length                    \u001b[1;36m12514\u001b[0m                   â”‚\n",
              "â”‚   Homogenous Documents      \u001b[3;92mTrue\u001b[0m                    â”‚\n",
              "â”‚   Has nested Documents in   \u001b[1m(\u001b[0m\u001b[32m'chunks'\u001b[0m,\u001b[1m)\u001b[0m             â”‚\n",
              "â”‚   Common Attributes         \u001b[1m(\u001b[0m\u001b[32m'id'\u001b[0m, \u001b[32m'chunks'\u001b[0m\u001b[1m)\u001b[0m        â”‚\n",
              "â”‚   Multimodal dataclass      \u001b[3;91mFalse\u001b[0m                   â”‚\n",
              "â”‚                                                     â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Attributes Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚                                                                    â”‚\n",
              "â”‚  \u001b[1m \u001b[0m\u001b[1mAttribute\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mData type      \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m#Unique values\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mHas empty value\u001b[0m\u001b[1m \u001b[0m  â”‚\n",
              "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\n",
              "â”‚   chunks      \u001b[1m(\u001b[0m\u001b[32m'ChunkArray'\u001b[0m,\u001b[1m)\u001b[0m   \u001b[1;36m12514\u001b[0m            \u001b[3;91mFalse\u001b[0m             â”‚\n",
              "â”‚   id          \u001b[1m(\u001b[0m\u001b[32m'str'\u001b[0m,\u001b[1m)\u001b[0m          \u001b[1;36m12514\u001b[0m            \u001b[3;91mFalse\u001b[0m             â”‚\n",
              "â”‚                                                                    â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_data = DocumentArray.pull('fashion-train-data-clip', show_progress=True)\n",
        "eval_data = DocumentArray.pull('fashion-eval-data-clip', show_progress=True)\n",
        "\n",
        "train_data.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Documents Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚                                                     â”‚\n",
              "â”‚   Type                      DocumentArrayInMemory   â”‚\n",
              "â”‚   Length                    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">787</span>                     â”‚\n",
              "â”‚   Homogenous Documents      <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>                    â”‚\n",
              "â”‚   Has nested Documents in   <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'chunks'</span>,<span style=\"font-weight: bold\">)</span>             â”‚\n",
              "â”‚   Common Attributes         <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chunks'</span><span style=\"font-weight: bold\">)</span>        â”‚\n",
              "â”‚   Multimodal dataclass      <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>                   â”‚\n",
              "â”‚                                                     â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Attributes Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚                                                                    â”‚\n",
              "â”‚  <span style=\"font-weight: bold\"> Attribute </span> <span style=\"font-weight: bold\"> Data type       </span> <span style=\"font-weight: bold\"> #Unique values </span> <span style=\"font-weight: bold\"> Has empty value </span>  â”‚\n",
              "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\n",
              "â”‚   chunks      <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'ChunkArray'</span>,<span style=\"font-weight: bold\">)</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">787</span>              <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>             â”‚\n",
              "â”‚   id          <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span>,<span style=\"font-weight: bold\">)</span>          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">787</span>              <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>             â”‚\n",
              "â”‚                                                                    â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Documents Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚                                                     â”‚\n",
              "â”‚   Type                      DocumentArrayInMemory   â”‚\n",
              "â”‚   Length                    \u001b[1;36m787\u001b[0m                     â”‚\n",
              "â”‚   Homogenous Documents      \u001b[3;92mTrue\u001b[0m                    â”‚\n",
              "â”‚   Has nested Documents in   \u001b[1m(\u001b[0m\u001b[32m'chunks'\u001b[0m,\u001b[1m)\u001b[0m             â”‚\n",
              "â”‚   Common Attributes         \u001b[1m(\u001b[0m\u001b[32m'id'\u001b[0m, \u001b[32m'chunks'\u001b[0m\u001b[1m)\u001b[0m        â”‚\n",
              "â”‚   Multimodal dataclass      \u001b[3;91mFalse\u001b[0m                   â”‚\n",
              "â”‚                                                     â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Attributes Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚                                                                    â”‚\n",
              "â”‚  \u001b[1m \u001b[0m\u001b[1mAttribute\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mData type      \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m#Unique values\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mHas empty value\u001b[0m\u001b[1m \u001b[0m  â”‚\n",
              "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\n",
              "â”‚   chunks      \u001b[1m(\u001b[0m\u001b[32m'ChunkArray'\u001b[0m,\u001b[1m)\u001b[0m   \u001b[1;36m787\u001b[0m              \u001b[3;91mFalse\u001b[0m             â”‚\n",
              "â”‚   id          \u001b[1m(\u001b[0m\u001b[32m'str'\u001b[0m,\u001b[1m)\u001b[0m          \u001b[1;36m787\u001b[0m              \u001b[3;91mFalse\u001b[0m             â”‚\n",
              "â”‚                                                                    â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "eval_data.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE87a5Nvwd7q"
      },
      "source": [
        "## Backbone model\n",
        "Currently, we support several CLIP variations from [open-clip](https://github.com/mlfoundations/open_clip) for text to image retrieval tasks.\n",
        "\n",
        "However, you can see all available models either in [choose backbone](https://finetuner.jina.ai/walkthrough/choose-backbone/) section or by calling `finetuner.describe_models()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81fh900Bxgkn"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Now that we have the training and evaluation datasets loaded as `DocumentArray`s and selected our model, we can start our fine-tuning run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UDcpfybOv1dh"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'create_run'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfinetuner\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m run \u001b[39m=\u001b[39m finetuner\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      4\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mopenai/clip-vit-base-patch32\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     train_data\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[1;32m      6\u001b[0m     eval_data\u001b[39m=\u001b[39;49meval_data,\n\u001b[1;32m      7\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m \u001b[39m1e-7\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     loss\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mCLIPLoss\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/fast/lib/python3.9/site-packages/hubble/__init__.py:48\u001b[0m, in \u001b[0;36mlogin_required.<locals>.arg_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     Client(jsonify\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mget_user_info()\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m AuthenticationRequiredError:\n\u001b[1;32m     50\u001b[0m     \u001b[39mraise\u001b[39;00m AuthenticationRequiredError(\n\u001b[1;32m     51\u001b[0m         response\u001b[39m=\u001b[39m{},\n\u001b[1;32m     52\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mJina auth token is not provided or has expired. \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m!r}\u001b[39;00m\u001b[39m requires login to Jina AI, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     53\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mplease do `jina auth login -f` or set env variable `JINA_AUTH_TOKEN` with correct token\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     54\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/fast/lib/python3.9/site-packages/finetuner/__init__.py:206\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, train_data, eval_data, run_name, description, experiment_name, model_options, loss, miner, miner_options, optimizer, optimizer_options, learning_rate, epochs, batch_size, callbacks, scheduler_step, freeze, output_dim, cpu, device, num_workers, to_onnx, csv_options, public)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m@login_required\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[1;32m    109\u001b[0m     model: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m     public: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    134\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Run:\n\u001b[1;32m    135\u001b[0m     \u001b[39m\"\"\"Start a finetuner run!\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[39m    :param model: The name of model to be fine-tuned. Run `finetuner.list_models()` or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39m       extremely slow and inefficient.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mreturn\u001b[39;00m ft\u001b[39m.\u001b[39;49mcreate_run(\n\u001b[1;32m    207\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    208\u001b[0m         train_data\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[1;32m    209\u001b[0m         eval_data\u001b[39m=\u001b[39;49meval_data,\n\u001b[1;32m    210\u001b[0m         run_name\u001b[39m=\u001b[39;49mrun_name,\n\u001b[1;32m    211\u001b[0m         description\u001b[39m=\u001b[39;49mdescription,\n\u001b[1;32m    212\u001b[0m         experiment_name\u001b[39m=\u001b[39;49mexperiment_name,\n\u001b[1;32m    213\u001b[0m         model_options\u001b[39m=\u001b[39;49mmodel_options,\n\u001b[1;32m    214\u001b[0m         loss\u001b[39m=\u001b[39;49mloss,\n\u001b[1;32m    215\u001b[0m         miner\u001b[39m=\u001b[39;49mminer,\n\u001b[1;32m    216\u001b[0m         miner_options\u001b[39m=\u001b[39;49mminer_options,\n\u001b[1;32m    217\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    218\u001b[0m         optimizer_options\u001b[39m=\u001b[39;49moptimizer_options,\n\u001b[1;32m    219\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    220\u001b[0m         epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    221\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    222\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    223\u001b[0m         scheduler_step\u001b[39m=\u001b[39;49mscheduler_step,\n\u001b[1;32m    224\u001b[0m         freeze\u001b[39m=\u001b[39;49mfreeze,\n\u001b[1;32m    225\u001b[0m         output_dim\u001b[39m=\u001b[39;49moutput_dim,\n\u001b[1;32m    226\u001b[0m         cpu\u001b[39m=\u001b[39;49mcpu,\n\u001b[1;32m    227\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    228\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    229\u001b[0m         to_onnx\u001b[39m=\u001b[39;49mto_onnx,\n\u001b[1;32m    230\u001b[0m         csv_options\u001b[39m=\u001b[39;49mcsv_options,\n\u001b[1;32m    231\u001b[0m         public\u001b[39m=\u001b[39;49mpublic,\n\u001b[1;32m    232\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/envs/fast/lib/python3.9/site-packages/hubble/__init__.py:48\u001b[0m, in \u001b[0;36mlogin_required.<locals>.arg_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     Client(jsonify\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mget_user_info()\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m AuthenticationRequiredError:\n\u001b[1;32m     50\u001b[0m     \u001b[39mraise\u001b[39;00m AuthenticationRequiredError(\n\u001b[1;32m     51\u001b[0m         response\u001b[39m=\u001b[39m{},\n\u001b[1;32m     52\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mJina auth token is not provided or has expired. \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m!r}\u001b[39;00m\u001b[39m requires login to Jina AI, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     53\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mplease do `jina auth login -f` or set env variable `JINA_AUTH_TOKEN` with correct token\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     54\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/fast/lib/python3.9/site-packages/finetuner/finetuner.py:179\u001b[0m, in \u001b[0;36mFinetuner.create_run\u001b[0;34m(self, model, train_data, eval_data, run_name, description, experiment_name, model_options, loss, miner, miner_options, optimizer, optimizer_options, learning_rate, epochs, batch_size, callbacks, scheduler_step, freeze, output_dim, cpu, device, num_workers, to_onnx, csv_options, public)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     experiment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_experiment(name\u001b[39m=\u001b[39mexperiment_name)\n\u001b[0;32m--> 179\u001b[0m \u001b[39mreturn\u001b[39;00m experiment\u001b[39m.\u001b[39;49mcreate_run(\n\u001b[1;32m    180\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    181\u001b[0m     train_data\u001b[39m=\u001b[39mtrain_data,\n\u001b[1;32m    182\u001b[0m     eval_data\u001b[39m=\u001b[39meval_data,\n\u001b[1;32m    183\u001b[0m     run_name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    184\u001b[0m     description\u001b[39m=\u001b[39mdescription,\n\u001b[1;32m    185\u001b[0m     model_options\u001b[39m=\u001b[39mmodel_options \u001b[39mor\u001b[39;00m {},\n\u001b[1;32m    186\u001b[0m     loss\u001b[39m=\u001b[39mloss,\n\u001b[1;32m    187\u001b[0m     miner\u001b[39m=\u001b[39mminer,\n\u001b[1;32m    188\u001b[0m     miner_options\u001b[39m=\u001b[39mminer_options,\n\u001b[1;32m    189\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[1;32m    190\u001b[0m     optimizer_options\u001b[39m=\u001b[39moptimizer_options,\n\u001b[1;32m    191\u001b[0m     learning_rate\u001b[39m=\u001b[39mlearning_rate,\n\u001b[1;32m    192\u001b[0m     epochs\u001b[39m=\u001b[39mepochs,\n\u001b[1;32m    193\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    194\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks \u001b[39mor\u001b[39;00m [],\n\u001b[1;32m    195\u001b[0m     scheduler_step\u001b[39m=\u001b[39mscheduler_step,\n\u001b[1;32m    196\u001b[0m     freeze\u001b[39m=\u001b[39mfreeze,\n\u001b[1;32m    197\u001b[0m     output_dim\u001b[39m=\u001b[39moutput_dim,\n\u001b[1;32m    198\u001b[0m     cpu\u001b[39m=\u001b[39mcpu,\n\u001b[1;32m    199\u001b[0m     device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m    200\u001b[0m     num_workers\u001b[39m=\u001b[39mnum_workers,\n\u001b[1;32m    201\u001b[0m     to_onnx\u001b[39m=\u001b[39mto_onnx,\n\u001b[1;32m    202\u001b[0m     csv_options\u001b[39m=\u001b[39mcsv_options,\n\u001b[1;32m    203\u001b[0m     public\u001b[39m=\u001b[39mpublic,\n\u001b[1;32m    204\u001b[0m )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'create_run'"
          ]
        }
      ],
      "source": [
        "import finetuner\n",
        "\n",
        "run = finetuner.fit(\n",
        "    model='openai/clip-vit-base-patch32',\n",
        "    train_data=train_data,\n",
        "    eval_data=eval_data,\n",
        "    epochs=5,\n",
        "    learning_rate= 1e-7,\n",
        "    loss='CLIPLoss',\n",
        "    device='cuda',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPDmFdubxzUE"
      },
      "source": [
        "Let's understand what this piece of code does:\n",
        "\n",
        "* We start with providing `model`, names of training and evaluation data.\n",
        "* We also provide some hyper-parameters such as number of `epochs` and a `learning_rate`.\n",
        "* We use `CLIPLoss` to optimize the CLIP model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKv3VcMKyG8d"
      },
      "source": [
        "## Monitoring\n",
        "\n",
        "Now that we've created a run, let's see its status. You can monitor the run by checking the status - `run.status()` and - the logs - `run.logs()` or - `run.stream_logs()`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX45y-2fxs4L"
      },
      "outputs": [],
      "source": [
        "# note, the fine-tuning might takes 20~ minutes\n",
        "for entry in run.stream_logs():\n",
        "    print(entry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi49YlQsyXbi"
      },
      "source": [
        "Since some runs might take up to several hours/days, it's important to know how to reconnect to Finetuner and retrieve your run.\n",
        "\n",
        "```python\n",
        "import finetuner\n",
        "\n",
        "finetuner.login()\n",
        "run = finetuner.get_run(run.name)\n",
        "```\n",
        "\n",
        "You can continue monitoring the run by checking the status - `finetuner.run.Run.status()` or the logs - `finetuner.run.Run.logs()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xeq_aVRxyqlW"
      },
      "source": [
        "## Evaluating\n",
        "Currently, we don't have a user-friendly way to get evaluation metrics from the {class}`~finetuner.callback.EvaluationCallback` we initialized previously.\n",
        "\n",
        "```bash\n",
        "           INFO     Done âœ¨                                                                              __main__.py:219\n",
        "           INFO     Saving fine-tuned models ...                                                         __main__.py:222\n",
        "           INFO     Saving model 'model' in /usr/src/app/tuned-models/model ...                          __main__.py:233\n",
        "           INFO     Pushing saved model to Jina AI Cloud ...                                                    __main__.py:240\n",
        "[10:38:14] INFO     Pushed model artifact ID: '62a1af491597c219f6a330fe'                                 __main__.py:246\n",
        "           INFO     Finished ğŸš€                                                                          __main__.py:248\n",
        "```\n",
        "\n",
        "```{admonition} Evaluation of CLIP\n",
        "\n",
        "In this example, we did not plug-in an `EvaluationCallback` since the callback can evaluate one model at one time.\n",
        "In most cases, we want to evaluate two models: i.e. use `CLIPTextEncoder` to encode textual Documents as `query_data` while use `CLIPImageEncoder` to encode image Documents as `index_data`.\n",
        "Then use the textual Documents to search image Documents.\n",
        "\n",
        "We have done the evaulation for you in the table below.\n",
        "```\n",
        "\n",
        "\n",
        "|                   | Before Finetuning | After Finetuning |\n",
        "|:------------------|---------:|---------:|\n",
        "| average_precision | 0.47219  | 0.532773 |\n",
        "| dcg_at_k          | 2.25565  | 2.70725  |\n",
        "| f1_score_at_k     | 0.296816 | 0.353499 |\n",
        "| hit_at_k          | 0.94028  | 0.942821 |\n",
        "| ndcg_at_k         | 0.613387 | 0.673644 |\n",
        "| precision_at_k    | 0.240407 | 0.285324 |\n",
        "| r_precision       | 0.364697 | 0.409577 |\n",
        "| recall_at_k       | 0.472681 | 0.564168 |\n",
        "| reciprocal_rank   | 0.575481 | 0.67571  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3qC3yAcy-Es"
      },
      "source": [
        "## Saving\n",
        "\n",
        "After the run has finished successfully, you can download the tuned model on your local machine:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sucF7touyKo0"
      },
      "outputs": [],
      "source": [
        "artifact = run.save_artifact('clip-model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_VGjKq3zDx9"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Now you saved the `artifact` into your host machine,\n",
        "let's use the fine-tuned model to encode a new `Document`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v95QsuEyzE-B"
      },
      "outputs": [],
      "source": [
        "text_da = DocumentArray([Document(text='some text to encode')])\n",
        "image_da = DocumentArray([Document(uri='https://upload.wikimedia.org/wikipedia/commons/4/4e/Single_apple.png')])\n",
        "\n",
        "clip_text_encoder = finetuner.get_model(artifact=artifact, select_model='clip-text')\n",
        "clip_image_encoder = finetuner.get_model(artifact=artifact, select_model='clip-vision')\n",
        "\n",
        "finetuner.encode(model=clip_text_encoder, data=text_da)\n",
        "finetuner.encode(model=clip_image_encoder, data=image_da)\n",
        "\n",
        "print(text_da.embeddings.shape)\n",
        "print(image_da.embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzMbR7VgzXtA"
      },
      "source": [
        "```bash\n",
        "(1, 512)\n",
        "(1, 512)\n",
        "```\n",
        "\n",
        "```{admonition} what is select_model?\n",
        "When fine-tuning CLIP, we are fine-tuning the CLIPVisionEncoder and CLIPTextEncoder in parallel.\n",
        "The artifact contains two models: `clip-vision` and `clip-text`.\n",
        "The parameter `select_model` tells finetuner which model to use for inference, in the above example,\n",
        "we use `clip-text` to encode a Document with text content.\n",
        "```\n",
        "\n",
        "```{admonition} Inference with ONNX\n",
        "In case you set `to_onnx=True` when calling `finetuner.fit` function,\n",
        "please use `model = finetuner.get_model(artifact, is_onnx=True)`\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHyMm_M1zxdt"
      },
      "source": [
        "## Advanced: WiSE-FT \n",
        "\n",
        "WiSE-FT, proposed by Mitchell et al. in [Robust fine-tuning of zero-shot models](https://arxiv.org/abs/2109.01903),\n",
        "has been proven to be an effective way for fine-tuning models with a strong zero-shot capability,\n",
        "such as CLIP.\n",
        "As was introduced in the paper:\n",
        "\n",
        "> Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT).\n",
        "\n",
        "Finetuner allows you to apply WiSE-FT easily,\n",
        "all you need to do is use the `WiSEFTCallback`.\n",
        "Finetuner will trigger the callback when the fine-tuning job is finished and merge the weights between the pre-trained model and the fine-tuned model:\n",
        "\n",
        "```diff\n",
        "from finetuner.callback import WiSEFTCallback\n",
        "\n",
        "run = finetuner.fit(\n",
        "    model='ViT-B-32#openai',\n",
        "    ...,\n",
        "    loss='CLIPLoss',\n",
        "-   callbacks=[],\n",
        "+   callbacks=[WiSEFTCallback(alpha=0.5)],\n",
        ")\n",
        "```\n",
        "\n",
        "The value you set to `alpha` should be greater equal than 0 and less equal than 1:\n",
        "\n",
        "+ if `alpha` is a float between 0 and 1, we merge the weights between the pre-trained model and the fine-tuned model.\n",
        "+ if `alpha` is 0, the fine-tuned model is identical to the pre-trained model.\n",
        "+ if `alpha` is 1, the pre-trained weights will not be utilized.\n",
        "\n",
        "\n",
        "That's it! Check out [clip-as-service](https://clip-as-service.jina.ai/user-guides/finetuner/?highlight=finetuner#fine-tune-models) to learn how to plug-in a fine-tuned CLIP model to our CLIP specific service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYmj0nozyVCL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('fast')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "91cf809ebeac7834cab322c0e9393d8c235d5aedcc8b94bfe1d55e7236ca8197"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

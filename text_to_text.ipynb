{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huf1E2zq7JWb"
      },
      "source": [
        "# Text-to-Text Search via BERT\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1Ui3Gw3ZL785I7AuzlHv3I0-jTvFFxJ4_?usp=sharing\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n",
        "\n",
        "Searching large amounts of text documents with text queries is a very popular use-case and Finetuner enables you to accomplish this easily.\n",
        "\n",
        "This guide will lead you through an example use-case to show you how Finetuner can be used for text to text retrieval (Dense Retrieval).\n",
        "\n",
        "*Note, please consider switching to GPU/TPU Runtime for faster inference.*\n",
        "\n",
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CSuWo72R7Sno",
        "outputId": "ab39a5d0-99d8-46ca-c290-45a0a915229b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting finetuner[full]\n",
            "  Downloading finetuner-0.6.5.tar.gz (32 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jina-hubble-sdk==0.23.3\n",
            "  Downloading jina_hubble_sdk-0.23.3-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting docarray[common]>=0.18.0\n",
            "  Downloading docarray-0.18.1.tar.gz (644 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 644 kB 33.5 MB/s \n",
            "\u001b[?25hCollecting finetuner-stubs==0.11.4\n",
            "  Downloading finetuner_stubs-0.11.4-py3-none-any.whl (10 kB)\n",
            "Collecting finetuner-commons==0.11.4\n",
            "  Downloading finetuner_commons-0.11.4-py3-none-any.whl (31 kB)\n",
            "Collecting albumentations==1.2.0\n",
            "  Downloading albumentations-1.2.0-py3-none-any.whl (113 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113 kB 62.5 MB/s \n",
            "\u001b[?25hCollecting onnx>1.11.0\n",
            "  Downloading onnx-1.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.1 MB 49.6 MB/s \n",
            "\u001b[?25hCollecting transformers==4.20.1\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.4 MB 47.0 MB/s \n",
            "\u001b[?25hCollecting onnxruntime>1.11.1\n",
            "  Downloading onnxruntime-1.13.1-cp37-cp37m-manylinux_2_27_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.5 MB 56.8 MB/s \n",
            "\u001b[?25hCollecting open-clip-torch==1.3.0\n",
            "  Downloading open_clip_torch-1.3.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4 MB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from finetuner-commons==0.11.4->finetuner[full]) (1.12.1+cu113)\n",
            "Collecting rich>=12.4.4\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 237 kB 63.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision~=0.13.0 in /usr/local/lib/python3.7/dist-packages (from finetuner-commons==0.11.4->finetuner[full]) (0.13.1+cu113)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.0->finetuner-commons==0.11.4->finetuner[full]) (4.6.0.66)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.0->finetuner-commons==0.11.4->finetuner[full]) (0.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.0->finetuner-commons==0.11.4->finetuner[full]) (1.7.3)\n",
            "Requirement already satisfied: scikit-image<0.19,>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.0->finetuner-commons==0.11.4->finetuner[full]) (0.18.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.0->finetuner-commons==0.11.4->finetuner[full]) (6.0)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.0->finetuner-commons==0.11.4->finetuner[full]) (1.21.6)\n",
            "Requirement already satisfied: pydantic~=1.9 in /usr/local/lib/python3.7/dist-packages (from finetuner-stubs==0.11.4->finetuner[full]) (1.10.2)\n",
            "Collecting pathspec\n",
            "  Downloading pathspec-0.10.2-py3-none-any.whl (28 kB)\n",
            "Collecting docker\n",
            "  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147 kB 71.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jina-hubble-sdk==0.23.3->finetuner[full]) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from jina-hubble-sdk==0.23.3->finetuner[full]) (3.8.3)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-38.0.3-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.1 MB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jina-hubble-sdk==0.23.3->finetuner[full]) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from jina-hubble-sdk==0.23.3->finetuner[full]) (3.8.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from open-clip-torch==1.3.0->finetuner-commons==0.11.4->finetuner[full]) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from open-clip-torch==1.3.0->finetuner-commons==0.11.4->finetuner[full]) (4.64.1)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163 kB 74.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.1->finetuner-commons==0.11.4->finetuner[full]) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.6 MB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.13.0 in /usr/local/lib/python3.7/dist-packages (from docarray[common]>=0.18.0->finetuner[full]) (3.19.6)\n",
            "Collecting lz4\n",
            "  Downloading lz4-4.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2 MB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from docarray[common]>=0.18.0->finetuner[full]) (3.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from docarray[common]>=0.18.0->finetuner[full]) (7.1.2)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.87.0-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.19.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1->finetuner-commons==0.11.4->finetuner[full]) (4.1.1)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime>1.11.1->finetuner-commons==0.11.4->finetuner[full]) (1.12)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from onnxruntime>1.11.1->finetuner-commons==0.11.4->finetuner[full]) (1.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.20.1->finetuner-commons==0.11.4->finetuner[full]) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.2.0->finetuner-commons==0.11.4->finetuner[full]) (1.0.2)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=12.4.4->finetuner-commons==0.11.4->finetuner[full]) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.2.0->finetuner-commons==0.11.4->finetuner[full]) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.2.0->finetuner-commons==0.11.4->finetuner[full]) (2.9.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.2.0->finetuner-commons==0.11.4->finetuner[full]) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.2.0->finetuner-commons==0.11.4->finetuner[full]) (2021.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->docarray[common]>=0.18.0->finetuner[full]) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->docarray[common]>=0.18.0->finetuner[full]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->docarray[common]>=0.18.0->finetuner[full]) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->docarray[common]>=0.18.0->finetuner[full]) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.0->finetuner-commons==0.11.4->finetuner[full]) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.0->finetuner-commons==0.11.4->finetuner[full]) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina-hubble-sdk==0.23.3->finetuner[full]) (2.1.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina-hubble-sdk==0.23.3->finetuner[full]) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina-hubble-sdk==0.23.3->finetuner[full]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina-hubble-sdk==0.23.3->finetuner[full]) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina-hubble-sdk==0.23.3->finetuner[full]) (1.8.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina-hubble-sdk==0.23.3->finetuner[full]) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina-hubble-sdk==0.23.3->finetuner[full]) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->jina-hubble-sdk==0.23.3->finetuner[full]) (22.1.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->jina-hubble-sdk==0.23.3->finetuner[full]) (2.10)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->jina-hubble-sdk==0.23.3->finetuner[full]) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->jina-hubble-sdk==0.23.3->finetuner[full]) (2.21)\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting urllib3>=1.26.0\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140 kB 71.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jina-hubble-sdk==0.23.3->finetuner[full]) (2022.9.24)\n",
            "Collecting starlette==0.21.0\n",
            "  Downloading starlette-0.21.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80 kB 10.7 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->open-clip-torch==1.3.0->finetuner-commons==0.11.4->finetuner[full]) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jina-hubble-sdk==0.23.3->finetuner[full]) (3.10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->onnxruntime>1.11.1->finetuner-commons==0.11.4->finetuner[full]) (1.2.1)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn->docarray[common]>=0.18.0->finetuner[full]) (7.1.2)\n",
            "Building wheels for collected packages: finetuner, docarray\n",
            "  Building wheel for finetuner (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for finetuner: filename=finetuner-0.6.5-py3-none-any.whl size=32527 sha256=812575bc5b32e1d19e692623317c8f4eee6a82f48bbf1bcc96ed9b6db531e48a\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/fe/9e/a5380e8953830863f84fe6a3776052e66c4b0b7739ee07dbd2\n",
            "  Building wheel for docarray (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docarray: filename=docarray-0.18.1-py3-none-any.whl size=697560 sha256=ed33750501622b1d1106fd32603d574d769d0bb04638a7cd4c24967e6ad802da\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/fa/43/4c5fd4d75d2eef5d6c4f463866963fbfda719ddfad57d61b75\n",
            "Successfully built finetuner docarray\n",
            "Installing collected packages: urllib3, websocket-client, sniffio, requests, commonmark, rich, pathspec, docker, cryptography, anyio, starlette, jina-hubble-sdk, humanfriendly, h11, uvicorn, tokenizers, lz4, huggingface-hub, ftfy, fastapi, docarray, coloredlogs, transformers, open-clip-torch, onnxruntime, onnx, finetuner-stubs, albumentations, finetuner-commons, finetuner\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.2.1\n",
            "    Uninstalling albumentations-1.2.1:\n",
            "      Successfully uninstalled albumentations-1.2.1\n",
            "Successfully installed albumentations-1.2.0 anyio-3.6.2 coloredlogs-15.0.1 commonmark-0.9.1 cryptography-38.0.3 docarray-0.18.1 docker-6.0.1 fastapi-0.87.0 finetuner-0.6.5 finetuner-commons-0.11.4 finetuner-stubs-0.11.4 ftfy-6.1.1 h11-0.14.0 huggingface-hub-0.10.1 humanfriendly-10.0 jina-hubble-sdk-0.23.3 lz4-4.0.2 onnx-1.12.0 onnxruntime-1.13.1 open-clip-torch-1.3.0 pathspec-0.10.2 requests-2.28.1 rich-12.6.0 sniffio-1.3.0 starlette-0.21.0 tokenizers-0.12.1 transformers-4.20.1 urllib3-1.26.12 uvicorn-0.19.0 websocket-client-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install 'finetuner[full]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPDhvWkw7kas"
      },
      "source": [
        "## Task\n",
        "\n",
        "In Finetuner, two BERT models are supported as backbones, namely `bert-base-cased` and `sentence-transformers/msmarco-distilbert-base-v3`, both of which are models hosted on Hugging Face.\n",
        "\n",
        "In this example, we will fine-tune `bert-base-cased` on the [Quora Question Pairs](https://www.sbert.net/examples/training/quora_duplicate_questions/README.html?highlight=quora#dataset) dataset, where the search task involves finding duplicate questions in the dataset. An example query for this search task might look as follows:\n",
        "\n",
        "```\n",
        "How can I be a good geologist?\n",
        "\n",
        "```\n",
        "\n",
        "Retrieved documents that could be duplicates for this question should then be ranked in the following order:\n",
        "\n",
        "```\n",
        "What should I do to be a great geologist?\n",
        "How do I become a geologist?\n",
        "What do geologists do?\n",
        "...\n",
        "\n",
        "```\n",
        "\n",
        "We can fine-tune BERT so that questions that are duplicates of each other are represented in close proximity and questions that are not duplicates will have representations that are further apart in the embedding space. In this way, we can rank the embeddings in our search space by their proximity to the query question and return the highest ranking duplicates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfR6g0E_8fOz"
      },
      "source": [
        "## Data\n",
        "\n",
        "We will use the [Quora Question Pairs](https://www.sbert.net/examples/training/quora_duplicate_questions/README.html?highlight=quora#dataset) dataset to show-case Finetuner for text to text search. We have already pre-processed this dataset and made it available for you to pull from Jina AI Cloud. Do this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pwS11Nsg7jPM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/aswin/anaconda3/envs/fast/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4427a64f94146348183c74383f33643",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value=\"\\n<div class='custom-container'>\\n    <style>\\n        .custom-container {\\n       â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import finetuner\n",
        "from docarray import DocumentArray, Document\n",
        "\n",
        "finetuner.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8PIO5T--p4tR"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸ” <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">You are logged in to Jina AI</span> as <span style=\"font-weight: bold\">masapasa</span>. To log out, use <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">jina auth logout</span>.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "ğŸ” \u001b[1;32mYou are logged in to Jina AI\u001b[0m as \u001b[1mmasapasa\u001b[0m. To log out, use \u001b[2mjina auth logout\u001b[0m.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78fdfcf2795e4f1eabd9eecb97aae3a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "056610cdb317413bbac1788b816dc992",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f06a2c21ff7347c6a1de85f81907c019",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Documents Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚                                                   â”‚\n",
              "â”‚   Type                   DocumentArrayInMemory    â”‚\n",
              "â”‚   Length                 <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">104598</span>                   â”‚\n",
              "â”‚   Homogenous Documents   <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>                     â”‚\n",
              "â”‚   Common Attributes      <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'tags'</span><span style=\"font-weight: bold\">)</span>   â”‚\n",
              "â”‚   Multimodal dataclass   <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>                    â”‚\n",
              "â”‚                                                   â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Attributes Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚                                                              â”‚\n",
              "â”‚  <span style=\"font-weight: bold\"> Attribute </span> <span style=\"font-weight: bold\"> Data type </span> <span style=\"font-weight: bold\"> #Unique values </span> <span style=\"font-weight: bold\"> Has empty value </span>  â”‚\n",
              "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\n",
              "â”‚   id          <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span>,<span style=\"font-weight: bold\">)</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">104598</span>           <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>             â”‚\n",
              "â”‚   tags        <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'dict'</span>,<span style=\"font-weight: bold\">)</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">104598</span>           <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>             â”‚\n",
              "â”‚   text        <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span>,<span style=\"font-weight: bold\">)</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">104559</span>           <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>             â”‚\n",
              "â”‚                                                              â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Documents Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚                                                   â”‚\n",
              "â”‚   Type                   DocumentArrayInMemory    â”‚\n",
              "â”‚   Length                 \u001b[1;36m104598\u001b[0m                   â”‚\n",
              "â”‚   Homogenous Documents   \u001b[3;92mTrue\u001b[0m                     â”‚\n",
              "â”‚   Common Attributes      \u001b[1m(\u001b[0m\u001b[32m'id'\u001b[0m, \u001b[32m'text'\u001b[0m, \u001b[32m'tags'\u001b[0m\u001b[1m)\u001b[0m   â”‚\n",
              "â”‚   Multimodal dataclass   \u001b[3;91mFalse\u001b[0m                    â”‚\n",
              "â”‚                                                   â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Attributes Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚                                                              â”‚\n",
              "â”‚  \u001b[1m \u001b[0m\u001b[1mAttribute\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mData type\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m#Unique values\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mHas empty value\u001b[0m\u001b[1m \u001b[0m  â”‚\n",
              "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\n",
              "â”‚   id          \u001b[1m(\u001b[0m\u001b[32m'str'\u001b[0m,\u001b[1m)\u001b[0m    \u001b[1;36m104598\u001b[0m           \u001b[3;91mFalse\u001b[0m             â”‚\n",
              "â”‚   tags        \u001b[1m(\u001b[0m\u001b[32m'dict'\u001b[0m,\u001b[1m)\u001b[0m   \u001b[1;36m104598\u001b[0m           \u001b[3;91mFalse\u001b[0m             â”‚\n",
              "â”‚   text        \u001b[1m(\u001b[0m\u001b[32m'str'\u001b[0m,\u001b[1m)\u001b[0m    \u001b[1;36m104559\u001b[0m           \u001b[3;91mFalse\u001b[0m             â”‚\n",
              "â”‚                                                              â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_data = DocumentArray.pull('quora_train.da', show_progress=True)\n",
        "query_data = DocumentArray.pull('quora_query_dev.da', show_progress=True)\n",
        "index_data = DocumentArray.pull('quora_index_dev.da', show_progress=True)\n",
        "\n",
        "train_data.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_IlEIp59g9v"
      },
      "source": [
        "So we have 104598 training `Document`s. Each `Document` consists of a text field that contains the question, as well as a `finetuner_label` which indicates the label to which the question belongs. If multiple questions have the same label, they are duplicates of one another. If they have different `finetuner_label`s, they have no duplicates of each other.\n",
        "\n",
        "As for the evaluation dataset, we load `query_data` and `index_data` separately. The `query_data` have the same structure as the `train_data`, consisting of labeled documents. The `index_data` are the data against which the queries will be matched, and contain many documents, some of which may be irrelevant to the queries (i.e. they have no duplicated in the `query_data`).\n",
        "If you look at the summaries for the `query_data` and `index_data`, you will find that they have the following number of samples:\n",
        "\n",
        "```\n",
        "Length of queries DocumentArray: 5000\n",
        "Length of index DocumentArray: 15746\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXYrABkN9vYO"
      },
      "source": [
        "## Backbone model\n",
        "To keep things simple, we have decided to fine-tune the BERT model `bert-base-cased`. We could also have chosen `sentence-transformers/msmarco-distilbert-base-v3` as our base model, which has already been fine-tuned on the MSMarco dataset. \n",
        "However, for the purpose of this experiment, we want to explore how much improvement in performance we can gain from fine-tuning `bert-base-cased` on the Quora Question Pairs dataset using Finetuner. \n",
        "Perhaps in the future, we might want to create another run where we experiment with fine-tuning other BERT models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAlQArUB99oG"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Now that we have the training and evaluation datasets loaded as `DocumentArray`s and selected our model, we can start our fine-tuning run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hsRfjf1Z8ymZ"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'create_run'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfinetuner\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallback\u001b[39;00m \u001b[39mimport\u001b[39;00m EvaluationCallback\n\u001b[0;32m----> 3\u001b[0m run \u001b[39m=\u001b[39m finetuner\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      4\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbert-base-cased\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     train_data\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mquora_train.da\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     loss\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTripletMarginLoss\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mAdam\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     learning_rate \u001b[39m=\u001b[39;49m \u001b[39m1e-5\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     13\u001b[0m         EvaluationCallback(\n\u001b[1;32m     14\u001b[0m             query_data\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mquora_query_dev.da\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m             index_data\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mquora_index_dev.da\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m             batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m\n\u001b[1;32m     17\u001b[0m         )\n\u001b[1;32m     18\u001b[0m     ]\n\u001b[1;32m     19\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/fast/lib/python3.9/site-packages/hubble/__init__.py:48\u001b[0m, in \u001b[0;36mlogin_required.<locals>.arg_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     Client(jsonify\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mget_user_info()\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m AuthenticationRequiredError:\n\u001b[1;32m     50\u001b[0m     \u001b[39mraise\u001b[39;00m AuthenticationRequiredError(\n\u001b[1;32m     51\u001b[0m         response\u001b[39m=\u001b[39m{},\n\u001b[1;32m     52\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mJina auth token is not provided or has expired. \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m!r}\u001b[39;00m\u001b[39m requires login to Jina AI, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     53\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mplease do `jina auth login -f` or set env variable `JINA_AUTH_TOKEN` with correct token\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     54\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/fast/lib/python3.9/site-packages/finetuner/__init__.py:206\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, train_data, eval_data, run_name, description, experiment_name, model_options, loss, miner, miner_options, optimizer, optimizer_options, learning_rate, epochs, batch_size, callbacks, scheduler_step, freeze, output_dim, cpu, device, num_workers, to_onnx, csv_options, public)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m@login_required\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[1;32m    109\u001b[0m     model: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m     public: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    134\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Run:\n\u001b[1;32m    135\u001b[0m     \u001b[39m\"\"\"Start a finetuner run!\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[39m    :param model: The name of model to be fine-tuned. Run `finetuner.list_models()` or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39m       extremely slow and inefficient.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mreturn\u001b[39;00m ft\u001b[39m.\u001b[39;49mcreate_run(\n\u001b[1;32m    207\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    208\u001b[0m         train_data\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[1;32m    209\u001b[0m         eval_data\u001b[39m=\u001b[39;49meval_data,\n\u001b[1;32m    210\u001b[0m         run_name\u001b[39m=\u001b[39;49mrun_name,\n\u001b[1;32m    211\u001b[0m         description\u001b[39m=\u001b[39;49mdescription,\n\u001b[1;32m    212\u001b[0m         experiment_name\u001b[39m=\u001b[39;49mexperiment_name,\n\u001b[1;32m    213\u001b[0m         model_options\u001b[39m=\u001b[39;49mmodel_options,\n\u001b[1;32m    214\u001b[0m         loss\u001b[39m=\u001b[39;49mloss,\n\u001b[1;32m    215\u001b[0m         miner\u001b[39m=\u001b[39;49mminer,\n\u001b[1;32m    216\u001b[0m         miner_options\u001b[39m=\u001b[39;49mminer_options,\n\u001b[1;32m    217\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    218\u001b[0m         optimizer_options\u001b[39m=\u001b[39;49moptimizer_options,\n\u001b[1;32m    219\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    220\u001b[0m         epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    221\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    222\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    223\u001b[0m         scheduler_step\u001b[39m=\u001b[39;49mscheduler_step,\n\u001b[1;32m    224\u001b[0m         freeze\u001b[39m=\u001b[39;49mfreeze,\n\u001b[1;32m    225\u001b[0m         output_dim\u001b[39m=\u001b[39;49moutput_dim,\n\u001b[1;32m    226\u001b[0m         cpu\u001b[39m=\u001b[39;49mcpu,\n\u001b[1;32m    227\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    228\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    229\u001b[0m         to_onnx\u001b[39m=\u001b[39;49mto_onnx,\n\u001b[1;32m    230\u001b[0m         csv_options\u001b[39m=\u001b[39;49mcsv_options,\n\u001b[1;32m    231\u001b[0m         public\u001b[39m=\u001b[39;49mpublic,\n\u001b[1;32m    232\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/envs/fast/lib/python3.9/site-packages/hubble/__init__.py:48\u001b[0m, in \u001b[0;36mlogin_required.<locals>.arg_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     Client(jsonify\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mget_user_info()\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m AuthenticationRequiredError:\n\u001b[1;32m     50\u001b[0m     \u001b[39mraise\u001b[39;00m AuthenticationRequiredError(\n\u001b[1;32m     51\u001b[0m         response\u001b[39m=\u001b[39m{},\n\u001b[1;32m     52\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mJina auth token is not provided or has expired. \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m!r}\u001b[39;00m\u001b[39m requires login to Jina AI, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     53\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mplease do `jina auth login -f` or set env variable `JINA_AUTH_TOKEN` with correct token\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     54\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/fast/lib/python3.9/site-packages/finetuner/finetuner.py:179\u001b[0m, in \u001b[0;36mFinetuner.create_run\u001b[0;34m(self, model, train_data, eval_data, run_name, description, experiment_name, model_options, loss, miner, miner_options, optimizer, optimizer_options, learning_rate, epochs, batch_size, callbacks, scheduler_step, freeze, output_dim, cpu, device, num_workers, to_onnx, csv_options, public)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     experiment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_experiment(name\u001b[39m=\u001b[39mexperiment_name)\n\u001b[0;32m--> 179\u001b[0m \u001b[39mreturn\u001b[39;00m experiment\u001b[39m.\u001b[39;49mcreate_run(\n\u001b[1;32m    180\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    181\u001b[0m     train_data\u001b[39m=\u001b[39mtrain_data,\n\u001b[1;32m    182\u001b[0m     eval_data\u001b[39m=\u001b[39meval_data,\n\u001b[1;32m    183\u001b[0m     run_name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    184\u001b[0m     description\u001b[39m=\u001b[39mdescription,\n\u001b[1;32m    185\u001b[0m     model_options\u001b[39m=\u001b[39mmodel_options \u001b[39mor\u001b[39;00m {},\n\u001b[1;32m    186\u001b[0m     loss\u001b[39m=\u001b[39mloss,\n\u001b[1;32m    187\u001b[0m     miner\u001b[39m=\u001b[39mminer,\n\u001b[1;32m    188\u001b[0m     miner_options\u001b[39m=\u001b[39mminer_options,\n\u001b[1;32m    189\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[1;32m    190\u001b[0m     optimizer_options\u001b[39m=\u001b[39moptimizer_options,\n\u001b[1;32m    191\u001b[0m     learning_rate\u001b[39m=\u001b[39mlearning_rate,\n\u001b[1;32m    192\u001b[0m     epochs\u001b[39m=\u001b[39mepochs,\n\u001b[1;32m    193\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    194\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks \u001b[39mor\u001b[39;00m [],\n\u001b[1;32m    195\u001b[0m     scheduler_step\u001b[39m=\u001b[39mscheduler_step,\n\u001b[1;32m    196\u001b[0m     freeze\u001b[39m=\u001b[39mfreeze,\n\u001b[1;32m    197\u001b[0m     output_dim\u001b[39m=\u001b[39moutput_dim,\n\u001b[1;32m    198\u001b[0m     cpu\u001b[39m=\u001b[39mcpu,\n\u001b[1;32m    199\u001b[0m     device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m    200\u001b[0m     num_workers\u001b[39m=\u001b[39mnum_workers,\n\u001b[1;32m    201\u001b[0m     to_onnx\u001b[39m=\u001b[39mto_onnx,\n\u001b[1;32m    202\u001b[0m     csv_options\u001b[39m=\u001b[39mcsv_options,\n\u001b[1;32m    203\u001b[0m     public\u001b[39m=\u001b[39mpublic,\n\u001b[1;32m    204\u001b[0m )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'create_run'"
          ]
        }
      ],
      "source": [
        "from finetuner.callback import EvaluationCallback\n",
        "\n",
        "run = finetuner.fit(\n",
        "    model='bert-base-cased',\n",
        "    train_data='quora_train.da',\n",
        "    loss='TripletMarginLoss',\n",
        "    optimizer='Adam',\n",
        "    learning_rate = 1e-5,\n",
        "    epochs=3,\n",
        "    batch_size=128,\n",
        "    device='cuda',\n",
        "    callbacks=[\n",
        "        EvaluationCallback(\n",
        "            query_data='quora_query_dev.da',\n",
        "            index_data='quora_index_dev.da',\n",
        "            batch_size=32\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_MxAW9E-ddZ"
      },
      "source": [
        "Our fine-tuning call has a lot of arguments. Let's discuss what the most important ones are responsible for. \n",
        "\n",
        "Most importantly, we select our model with `model='bert-base-cased'` and pass our training data with `train_data=train_data`. These two arguments are required. \n",
        "We set our `experiment_name` to `'finetune-quora-dataset'` and our `run_name` to `'finetune-quora-dataset-bert-base-cased'`. \n",
        "This will make it easy for us to retrieve the experiment and run in the future. We also provide a short description of our run, just for some extra context. \n",
        "\n",
        "For this run, we select Finetuner's `TripletMarginLoss` and `TripletMarginMiner`, as they are most relevant for our use-case. The `TripletMarginLoss` measures the similarity between three tensors, namely the anchor, a positive sample and a negative sample. This makes sense for our task, since we want duplicate questions to have representations closer together, while non-duplicates should have more dissimilar representations. Likewise, the `TripletMarginMiner` outputs a tuple of size 3, with an anchor, a positive sample and a negative sample.\n",
        "\n",
        "Lastly, we provide an `EvaluationCallback` with our `query_data` and `index_data`. This evaluation is done at the end of each epoch and its results will be visible to us in the logs, which we will monitor in the next section. Since we have not specified which metrics should be applied, default metrics will be computed. The `Evaluation` section of this guide will show you the default metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0DGNRo8-lZD"
      },
      "source": [
        "## Monitoring\n",
        "\n",
        "Now that we've created a run, let's see its status. You can monitor the run by checking the status - `run.status()`, -and the logs `run.logs()` or `run.stream_logs()`. \n",
        "\n",
        "*note, the job will take around 15 minutes to finish.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gajka0TG-S6u"
      },
      "outputs": [],
      "source": [
        "for entry in run.stream_logs():\n",
        "    print(entry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AuB0IWC_CSt"
      },
      "source": [
        "Dependending on the size of the training data, some runs might take up to several hours. You can later reconnect to your run very easily to monitor its status.\n",
        "\n",
        "```python\n",
        "import finetuner\n",
        "\n",
        "finetuner.login()\n",
        "run = finetuner.get_run('finetune-quora-dataset-bert-base-cased')\n",
        "print(f'Run status: {run.status()}')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agqrb0TX_Y4b"
      },
      "source": [
        "## Evaluating\n",
        "\n",
        "Our `EvaluationCallback` during fine-tuning ensures that after each epoch, an evaluation of our model is run. We can access the evaluation results in the logs as follows `print(run.logs())`:\n",
        "\n",
        "```bash\n",
        "  Training [3/3] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 818/818 0:00:00 0:03:05 â€¢ loss: 0.000\n",
        "[15:36:40] DEBUG    Metric: 'model_average_precision' Value: 0.95728                                     __main__.py:202\n",
        "           DEBUG    Metric: 'model_dcg_at_k' Value: 1.33912                                              __main__.py:202\n",
        "           DEBUG    Metric: 'model_f1_score_at_k' Value: 0.13469                                         __main__.py:202\n",
        "           DEBUG    Metric: 'model_hit_at_k' Value: 0.99720                                              __main__.py:202\n",
        "           DEBUG    Metric: 'model_ndcg_at_k' Value: 0.97529                                             __main__.py:202\n",
        "           DEBUG    Metric: 'model_precision_at_k' Value: 0.07653                                        __main__.py:202\n",
        "           DEBUG    Metric: 'model_r_precision' Value: 0.94393                                           __main__.py:202\n",
        "           DEBUG    Metric: 'model_recall_at_k' Value: 0.99301                                           __main__.py:202\n",
        "           DEBUG    Metric: 'model_reciprocal_rank' Value: 0.96686                                       __main__.py:202\n",
        "           INFO     Done âœ¨                                                                              __main__.py:204\n",
        "           INFO     Saving fine-tuned models ...                                                         __main__.py:207\n",
        "           INFO     Saving model 'model' in /usr/src/app/tuned-models/model ...                          __main__.py:218\n",
        "[15:36:41] INFO     Pushing saved model to Jina AI Cloud ...                                                    __main__.py:225\n",
        "[15:37:32] INFO     Pushed model artifact ID: '62b9cb73a411d7e08d18bd16'                                 __main__.py:231\n",
        "           INFO     Finished ğŸš€                                                                          __main__.py:233                                                  __main__.py:225\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTfBfB8A_1fO"
      },
      "source": [
        "## Saving\n",
        "Once your run has successfully completed, you can save your fine-tuned model in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7AJw3X9-7C-"
      },
      "outputs": [],
      "source": [
        "artifact = run.save_artifact('bert-model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYgPIR_kAI6z"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Now you saved the `artifact` into your host machine,\n",
        "let's use the fine-tuned model to encode a new `Document`:\n",
        "\n",
        "```{admonition} Inference with ONNX\n",
        "In case you set `to_onnx=True` when calling `finetuner.fit` function,\n",
        "please use `model = finetuner.get_model(artifact, is_onnx=True)`\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs2G-rNFAJ4I"
      },
      "outputs": [],
      "source": [
        "model = finetuner.get_model(artifact=artifact, device='cuda')\n",
        "\n",
        "query = DocumentArray([Document(text='How can I be an engineer?')])\n",
        "\n",
        "finetuner.encode(model=model, data=query)\n",
        "finetuner.encode(model=model, data=index_data)\n",
        "assert query.embeddings.shape == (1, 768)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_vUDidVIkh7"
      },
      "source": [
        "And finally you can use the embeded `query` to find top-k semantically related text within `index_data` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_bM-TXRE2h7"
      },
      "outputs": [],
      "source": [
        "query.match(index_data, limit=10, metric='cosine')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czK5pSUEAcdS"
      },
      "source": [
        "That's it!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('fast')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "91cf809ebeac7834cab322c0e9393d8c235d5aedcc8b94bfe1d55e7236ca8197"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
